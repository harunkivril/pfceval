{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "471ff0c7",
   "metadata": {},
   "source": [
    "### Base evaluation template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5250f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pfceval\n",
    "import polars as pl\n",
    "\n",
    "INDEX_COLS = [\"valid_time\", \"station_id\", \"step\"]\n",
    "EVAL_THS = [5, 10]\n",
    "CONSTRUCT_EVAL_REPORTS = True\n",
    "SAVE_DIR = \".\"\n",
    "\n",
    "def get_base_evaluation_report(forecast, experiment_name):\n",
    "\n",
    "    calc = pfceval.Calculator(forecast, INDEX_COLS)\n",
    "    calc.add_absolute_error()\n",
    "    calc.add_squared_error()\n",
    "    # Add probabilistic metrics\n",
    "    calc.add_spread()\n",
    "    calc.add_crps()\n",
    "    # Add threshold based metrics\n",
    "    for th in EVAL_THS:\n",
    "        calc.add_twcrps(th)\n",
    "        calc.add_brier(th)\n",
    "\n",
    "    report = pfceval.Evaluation.fill_evaluation(\n",
    "        experiment_name=experiment_name,\n",
    "        calculator=calc, \n",
    "        lead_time_col=\"step\",\n",
    "        location_id_col=\"station_id\",\n",
    "        bootstrap=True, \n",
    "        n_iter=1000, \n",
    "        CI=0.9,\n",
    "        location_metrics=True,\n",
    "    )\n",
    "\n",
    "    return report\n",
    "\n",
    "if CONSTRUCT_EVAL_REPORTS:\n",
    "    forecast_paths = pfceval.utils.get_example_forecast_paths()\n",
    "    for i, forecast_path in enumerate(forecast_paths):\n",
    "\n",
    "        forecast = pfceval.Forecast(\n",
    "            fc_path=forecast_path,\n",
    "            ensemble_prefix=\"pred_q\",\n",
    "            obs_col=\"wind_speed\",\n",
    "            bootstrap_cols=\"run_id\",\n",
    "        )\n",
    "\n",
    "        exp_name = f\"BaseEvaluationTemplate_{i}\"\n",
    "        report = get_base_evaluation_report(\n",
    "            forecast=forecast, \n",
    "            experiment_name=exp_name\n",
    "        )\n",
    "        # Get metrics for the stations that is present in training data\n",
    "        seen_report = get_base_evaluation_report(\n",
    "            forecast=forecast.filter(pl.col(\"unseen_sta\") == False), \n",
    "            experiment_name=exp_name\n",
    "        )\n",
    "        # Get metrics for the stations that is not present the training data\n",
    "        unseen_report = get_base_evaluation_report(\n",
    "            forecast=forecast.filter(pl.col(\"unseen_sta\") == True), \n",
    "            experiment_name=exp_name\n",
    "        )\n",
    "        # Get a common report for all filters\n",
    "        report.extend(seen_report, \"seen\")\n",
    "        report.extend(unseen_report, \"unseen\")\n",
    "        # Save the report to disk\n",
    "        report.save_results(f\"{SAVE_DIR}/{exp_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53be006",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_names = [\"BaseEvaluationTemplate_1\", \"BaseEvaluationTemplate_2\"]\n",
    "reports = [pfceval.Evaluation.load_report(f\"./{path}\" for path in exp_names)]\n",
    "\n",
    "overall = pfceval.plotting.stack_overall_metrics(*reports)\n",
    "overall.sort(\"mae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd1ef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_overall = pfceval.plotting.stack_overall_metrics(reports, table_name=\"unseen_overall_metrics\")\n",
    "unseen_overall.sort(\"mae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ee0345",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_overall = pfceval.plotting.stack_overall_metrics(reports, table_name=\"seen_overall_metrics\")\n",
    "seen_overall.sort(\"mae\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7efbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "pfceval.plotting.plot_lead_time_metrics(*reports, table_name=\"unseen_bootstraped_lead_time_metrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab081853",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
